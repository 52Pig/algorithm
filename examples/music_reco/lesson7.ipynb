{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/tf.jpeg)\n",
    "\n",
    "真实的场景中，可能我们有非常非常多的训练数据，我们不得不面对一些问题，也是大家比较关心的问题。\n",
    "\n",
    "1）海量的数据无法一次载入内存用于训练。<br>\n",
    "2）数据是每天不断增加的，我们有没有一些增量训练的方式去不断持续迭代更新模型？\n",
    "\n",
    "什么场景下，我们是不把数据全部载入内存优化，而是一个batch一个batch输入进行update参数的？<br>\n",
    "对，我们用tensorflow来完成一个在批量数据上更新，并且可以增量迭代优化的矩阵分解推荐系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.矩阵分解回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/svd_recommendation.png)\n",
    "LFM：把用户再item上打分的行为，看作是有内部依据的，认为和k个factor有关系<br>\n",
    "每一个user i会有一个用户的向量(k维)，每一个item会有一个item的向量(k维)\n",
    "\n",
    "SVD是矩阵分解的一种方式\n",
    "\n",
    "### 预测公式如下\n",
    "$y_{pred[u, i]} = bias_{global} + bias_{user[u]} + bias_{item_[i]} + <embedding_{user[u]}, embedding_{item[i]}>$\n",
    "\n",
    "### 我们需要最小化的loss计算如下（添加正则化项）\n",
    "$\\sum_{u, i} |y_{pred[u, i]} - y_{true[u, i]}|^2 + \\lambda(|embedding_{user[u]}|^2 + |embedding_{item[i]}|^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.获取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们依旧以movielens为例，数据格式为**user item rating timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这部分代码大家不用跑，因为数据已经下载好了\n",
    "#!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "#!sudo unzip ml-1m.zip -d ./movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据处理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们写点代码完成数据的产出和预处理过程。<br>\n",
    "大家知道tensorflow搭建的模型，训练方式通常是一个batch一个batch训练的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_data_and_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "class ShuffleDataIterator(object):\n",
    "    \"\"\"\n",
    "    随机生成一个batch一个batch数据\n",
    "    \"\"\"\n",
    "    #初始化\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    #总样本量\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    #取出下一个batch\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    #随机生成batch_size个下标，取出对应的样本\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochDataIterator(ShuffleDataIterator):\n",
    "    \"\"\"\n",
    "    顺序产出一个epoch的数据，在测试中可能会用到\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochDataIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们按照下图的方式用tensorflow去搭建一个可增量训练的矩阵分解模型，完成基于矩阵分解的推荐系统。\n",
    "![](./pic/tf_svd_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 使用矩阵分解搭建的网络结构\n",
    "def inference_svd(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    #使用CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # 初始化几个bias项\n",
    "        global_bias = tf.get_variable(\"global_bias\", shape=[])\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # bias向量\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # user向量与item向量\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    with tf.device(device):\n",
    "        # 按照实际公式进行计算\n",
    "        # 先对user向量和item向量求内积\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        # 加上几个偏置项\n",
    "        infer = tf.add(infer, global_bias)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # 加上正则化项\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), name=\"svd_regularizer\")\n",
    "    return infer, regularizer\n",
    "\n",
    "# 迭代优化部分\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.1, device=\"/cpu:0\"):\n",
    "    global_step = tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "    # 选择合适的optimizer做优化\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.数据上的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "\n",
    "np.random.seed(13575)\n",
    "\n",
    "# 一批数据的大小\n",
    "BATCH_SIZE = 2000\n",
    "# 用户数\n",
    "USER_NUM = 6040\n",
    "# 电影数\n",
    "ITEM_NUM = 3952\n",
    "# factor维度\n",
    "DIM = 15\n",
    "# 最大迭代轮数\n",
    "EPOCH_MAX = 200\n",
    "# 使用cpu做训练\n",
    "DEVICE = \"/cpu:0\"\n",
    "\n",
    "# 截断\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)\n",
    "\n",
    "# 这个是方便Tensorboard可视化做的summary\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=val)])\n",
    "\n",
    "# 调用上面的函数获取数据\n",
    "def get_data():\n",
    "    df = read_data_and_process(\"./data/movielens/ml-1m/ratings.dat\", sep=\"::\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * 0.9)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    return df_train, df_test\n",
    "\n",
    "# 实际训练过程\n",
    "def svd(train, test):\n",
    "    samples_per_batch = len(train) // BATCH_SIZE\n",
    "\n",
    "    # 一批一批数据用于训练\n",
    "    iter_train = ShuffleDataIterator([train[\"user\"],\n",
    "                                         train[\"item\"],\n",
    "                                         train[\"rate\"]],\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "    # 测试数据\n",
    "    iter_test = OneEpochDataIterator([test[\"user\"],\n",
    "                                         test[\"item\"],\n",
    "                                         test[\"rate\"]],\n",
    "                                        batch_size=-1)\n",
    "    # user和item batch\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    # 构建graph和训练\n",
    "    infer, regularizer = inference_svd(user_batch, item_batch, user_num=USER_NUM, item_num=ITEM_NUM, dim=DIM,\n",
    "                                           device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.05, device=DEVICE)\n",
    "\n",
    "    # 初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 开始迭代\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"/tmp/svd/log\", graph=sess.graph)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   item_batch: items,\n",
    "                                                                   rate_batch: rates})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items})\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print(\"{:3d} {:f} {:f} {:f}(s)\".format(i // samples_per_batch, train_err, test_err,\n",
    "                                                       end - start))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\", train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\", test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900188, 4) (100021, 4)\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "df_train, df_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-ccdefbf15c22>:65: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch train_error val_error elapsed_time\n",
      "  0 2.809092 2.814245 0.253496(s)\n",
      "  1 2.455195 1.437709 1.153895(s)\n",
      "  2 1.115765 0.986642 1.117186(s)\n",
      "  3 0.947091 0.935688 1.169409(s)\n",
      "  4 0.918870 0.921179 1.166361(s)\n",
      "  5 0.911003 0.915031 1.154213(s)\n",
      "  6 0.904026 0.911078 1.111087(s)\n",
      "  7 0.901008 0.907418 1.123565(s)\n",
      "  8 0.895782 0.903293 1.110647(s)\n",
      "  9 0.889292 0.899677 1.125567(s)\n",
      " 10 0.885593 0.896608 1.133308(s)\n",
      " 11 0.883175 0.893978 1.134873(s)\n",
      " 12 0.880212 0.891399 1.184469(s)\n",
      " 13 0.876919 0.889670 1.179899(s)\n",
      " 14 0.874496 0.887601 1.169226(s)\n",
      " 15 0.873145 0.886389 1.171524(s)\n",
      " 16 0.869967 0.884813 1.175456(s)\n",
      " 17 0.869573 0.883511 1.246640(s)\n",
      " 18 0.868100 0.882320 1.272476(s)\n",
      " 19 0.866781 0.880765 1.295218(s)\n",
      " 20 0.863130 0.879597 1.198241(s)\n",
      " 21 0.861313 0.877641 1.139313(s)\n",
      " 22 0.858756 0.875675 1.137141(s)\n",
      " 23 0.854028 0.873275 1.181878(s)\n",
      " 24 0.850513 0.870585 1.219393(s)\n",
      " 25 0.847186 0.868136 1.279292(s)\n",
      " 26 0.841954 0.865613 1.216062(s)\n",
      " 27 0.837368 0.863525 1.162821(s)\n",
      " 28 0.832156 0.861019 1.171520(s)\n",
      " 29 0.828571 0.858944 1.216725(s)\n",
      " 30 0.822632 0.857136 1.272917(s)\n",
      " 31 0.818658 0.854850 1.157441(s)\n",
      " 32 0.812013 0.853043 1.146976(s)\n",
      " 33 0.809315 0.851600 1.163579(s)\n",
      " 34 0.805383 0.850415 1.312787(s)\n",
      " 35 0.802480 0.849286 1.206877(s)\n",
      " 36 0.798687 0.848413 1.249514(s)\n",
      " 37 0.794845 0.847451 1.207445(s)\n",
      " 38 0.791780 0.846796 1.197213(s)\n",
      " 39 0.789386 0.846193 1.307452(s)\n",
      " 40 0.786481 0.845695 1.257362(s)\n",
      " 41 0.783477 0.845029 1.199003(s)\n",
      " 42 0.780574 0.844913 1.211698(s)\n",
      " 43 0.779103 0.844906 1.160053(s)\n",
      " 44 0.777583 0.844866 1.158339(s)\n",
      " 45 0.774328 0.844677 1.140164(s)\n",
      " 46 0.773697 0.844630 1.283957(s)\n",
      " 47 0.771151 0.844492 1.354236(s)\n",
      " 48 0.770198 0.844251 1.272542(s)\n",
      " 49 0.768271 0.844294 1.149203(s)\n",
      " 50 0.766511 0.844319 1.200569(s)\n",
      " 51 0.764832 0.844656 1.313089(s)\n",
      " 52 0.763985 0.844893 1.292719(s)\n",
      " 53 0.762422 0.845146 1.190800(s)\n",
      " 54 0.761935 0.845406 1.244993(s)\n",
      " 55 0.760223 0.845506 1.185580(s)\n",
      " 56 0.759554 0.845584 1.262621(s)\n",
      " 57 0.758891 0.846148 1.252910(s)\n",
      " 58 0.757887 0.846221 1.128272(s)\n",
      " 59 0.758388 0.846188 1.186030(s)\n",
      " 60 0.755421 0.846306 1.139265(s)\n",
      " 61 0.755574 0.846530 1.139090(s)\n",
      " 62 0.755502 0.846394 1.228908(s)\n",
      " 63 0.753591 0.846670 1.237165(s)\n",
      " 64 0.754389 0.846680 1.153077(s)\n",
      " 65 0.753793 0.846979 1.131575(s)\n",
      " 66 0.752217 0.846979 1.142809(s)\n",
      " 67 0.752888 0.847340 1.139158(s)\n",
      " 68 0.751561 0.847442 1.165792(s)\n",
      " 69 0.751461 0.847239 1.132730(s)\n",
      " 70 0.750680 0.847342 1.157513(s)\n",
      " 71 0.751546 0.847269 1.259253(s)\n",
      " 72 0.750221 0.847199 1.293315(s)\n",
      " 73 0.750541 0.847277 1.240448(s)\n",
      " 74 0.749487 0.847512 1.323953(s)\n",
      " 75 0.750102 0.847799 1.207037(s)\n",
      " 76 0.750167 0.847894 1.147260(s)\n",
      " 77 0.749048 0.848106 1.174262(s)\n",
      " 78 0.749985 0.847967 1.188354(s)\n",
      " 79 0.749156 0.847931 1.147350(s)\n",
      " 80 0.747979 0.848033 1.138077(s)\n",
      " 81 0.747604 0.848328 1.140339(s)\n",
      " 82 0.747730 0.848511 1.143999(s)\n",
      " 83 0.748759 0.848531 1.127257(s)\n",
      " 84 0.747835 0.848578 1.143956(s)\n",
      " 85 0.746049 0.848837 1.201335(s)\n",
      " 86 0.746963 0.848876 1.338047(s)\n",
      " 87 0.746619 0.848733 1.171926(s)\n",
      " 88 0.746442 0.848823 1.249340(s)\n",
      " 89 0.747331 0.848946 1.160565(s)\n",
      " 90 0.745484 0.848821 1.139195(s)\n",
      " 91 0.746499 0.848733 1.147395(s)\n",
      " 92 0.746571 0.848756 1.202760(s)\n",
      " 93 0.745974 0.848763 1.180345(s)\n",
      " 94 0.746077 0.848804 1.133779(s)\n",
      " 95 0.746416 0.848766 1.163294(s)\n",
      " 96 0.744625 0.848551 1.158206(s)\n",
      " 97 0.744683 0.848775 1.174434(s)\n",
      " 98 0.744663 0.849028 1.218583(s)\n",
      " 99 0.744656 0.848922 1.180774(s)\n",
      "100 0.744643 0.848946 1.226284(s)\n",
      "101 0.744534 0.849078 1.168176(s)\n",
      "102 0.744698 0.849128 1.205440(s)\n",
      "103 0.745651 0.849179 1.230591(s)\n",
      "104 0.745082 0.849430 1.262610(s)\n",
      "105 0.743996 0.849303 1.319508(s)\n",
      "106 0.744653 0.849104 1.239103(s)\n",
      "107 0.744702 0.849260 1.266356(s)\n",
      "108 0.745813 0.849210 1.300363(s)\n",
      "109 0.744137 0.849388 1.237682(s)\n",
      "110 0.745242 0.849438 1.248421(s)\n",
      "111 0.743160 0.849726 1.160946(s)\n",
      "112 0.743898 0.849671 1.136960(s)\n",
      "113 0.743669 0.849570 1.134645(s)\n",
      "114 0.744371 0.849395 1.153311(s)\n",
      "115 0.743538 0.849447 1.126135(s)\n",
      "116 0.743441 0.849409 1.136873(s)\n",
      "117 0.744142 0.849473 1.164846(s)\n",
      "118 0.742701 0.849407 1.146711(s)\n",
      "119 0.744080 0.849496 1.253328(s)\n",
      "120 0.743984 0.849540 1.356904(s)\n",
      "121 0.743296 0.849321 1.359798(s)\n",
      "122 0.744456 0.849430 1.363462(s)\n",
      "123 0.743654 0.849454 1.204117(s)\n",
      "124 0.743843 0.849298 1.161764(s)\n",
      "125 0.742814 0.849728 1.202416(s)\n",
      "126 0.744108 0.849300 1.129216(s)\n",
      "127 0.742565 0.849304 1.124533(s)\n",
      "128 0.742971 0.849315 1.242525(s)\n",
      "129 0.743007 0.849409 1.264886(s)\n",
      "130 0.743339 0.849449 1.213498(s)\n",
      "131 0.742480 0.849558 1.317492(s)\n",
      "132 0.743173 0.849324 1.251717(s)\n",
      "133 0.742660 0.849349 1.219554(s)\n",
      "134 0.743306 0.849254 1.249542(s)\n",
      "135 0.743259 0.849511 1.245479(s)\n",
      "136 0.742455 0.849615 1.256221(s)\n",
      "137 0.742117 0.849766 1.224056(s)\n",
      "138 0.743327 0.849928 1.223579(s)\n",
      "139 0.742563 0.849776 1.237772(s)\n",
      "140 0.742218 0.849782 1.249154(s)\n",
      "141 0.743734 0.849891 1.216273(s)\n",
      "142 0.742565 0.849850 1.224735(s)\n",
      "143 0.742298 0.849648 1.294835(s)\n",
      "144 0.743598 0.849593 1.285235(s)\n",
      "145 0.741487 0.849586 1.225003(s)\n",
      "146 0.742510 0.849704 1.257155(s)\n",
      "147 0.741807 0.849711 1.251258(s)\n",
      "148 0.742128 0.849916 1.248403(s)\n",
      "149 0.742504 0.849891 1.246339(s)\n",
      "150 0.742736 0.850028 1.268375(s)\n",
      "151 0.742441 0.850045 1.299802(s)\n",
      "152 0.742213 0.849976 1.257516(s)\n",
      "153 0.741962 0.849858 1.254810(s)\n",
      "154 0.742877 0.849908 1.241192(s)\n",
      "155 0.741119 0.849888 1.359833(s)\n",
      "156 0.742365 0.850086 1.405925(s)\n",
      "157 0.742530 0.849971 1.380889(s)\n",
      "158 0.741109 0.850040 1.178367(s)\n",
      "159 0.742575 0.849747 1.190400(s)\n",
      "160 0.742233 0.849886 1.162486(s)\n",
      "161 0.742248 0.849779 1.251139(s)\n",
      "162 0.740984 0.849924 1.195984(s)\n",
      "163 0.741714 0.849886 1.202207(s)\n",
      "164 0.741434 0.849729 1.166551(s)\n",
      "165 0.741614 0.849850 1.210318(s)\n",
      "166 0.743192 0.849963 1.306352(s)\n",
      "167 0.741990 0.849870 1.180140(s)\n",
      "168 0.740944 0.849806 1.207652(s)\n",
      "169 0.742299 0.849673 1.295902(s)\n",
      "170 0.741996 0.849808 1.316185(s)\n",
      "171 0.741630 0.849697 1.292256(s)\n",
      "172 0.742068 0.849815 1.162787(s)\n",
      "173 0.741633 0.849613 1.246128(s)\n",
      "174 0.741314 0.849628 1.207205(s)\n",
      "175 0.741315 0.849513 1.180957(s)\n",
      "176 0.741057 0.849690 1.233342(s)\n",
      "177 0.742114 0.849741 1.227712(s)\n",
      "178 0.741477 0.849597 1.315255(s)\n",
      "179 0.741489 0.849679 1.193022(s)\n",
      "180 0.742133 0.849776 1.173914(s)\n",
      "181 0.741000 0.849846 1.357603(s)\n",
      "182 0.741359 0.849820 1.241890(s)\n",
      "183 0.742044 0.849874 1.385900(s)\n",
      "184 0.741641 0.849943 1.391358(s)\n",
      "185 0.741127 0.849913 1.390963(s)\n",
      "186 0.743062 0.850013 1.367108(s)\n",
      "187 0.742271 0.849920 1.271825(s)\n",
      "188 0.741234 0.849897 1.131382(s)\n",
      "189 0.741536 0.849673 1.162062(s)\n",
      "190 0.741611 0.849846 1.173022(s)\n",
      "191 0.740363 0.849844 1.225228(s)\n",
      "192 0.741495 0.849993 1.272079(s)\n",
      "193 0.741053 0.849766 1.174174(s)\n",
      "194 0.742094 0.849685 1.167787(s)\n",
      "195 0.741693 0.849702 1.174942(s)\n",
      "196 0.742316 0.849793 1.358346(s)\n",
      "197 0.741218 0.849642 1.310413(s)\n",
      "198 0.740341 0.849713 1.221618(s)\n",
      "199 0.740397 0.849802 1.234791(s)\n"
     ]
    }
   ],
   "source": [
    "# 完成实际的训练\n",
    "svd(df_train, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
